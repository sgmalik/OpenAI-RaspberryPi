**OpenAI With Raspberry Pi** API KEY DISABLED

With the rise of AI and the increasing popularity of chatbots, our group decided to design our own using the openAI interface. Our goal is to create an interactive unit with its own UI, controllable by an LCD touchscreen that allows users to ask questions which will be answered by our implemented AI. We plan for this unit to be used by students as a study tool as well as anyone who is looking for an alternative to chatbots created by companies like Amazon and Google. In order to combat students utilizing AI to cheat on their assignments, we hope to implement user authentication to monitor and control user access to the openAI server. With the increasing price of computers and other various pieces of technology, we wanted to design something that allows those without these technologies to access the power of AI, without the need for expensive hardware. 

As we stated in our introduction, our project is an individual unit that acts as a chatbot with user authentication. Breaking the project down, component by component, we have:

LCD: This was implemented using software and a driver that we obtained when purchasing the screen. We’ve linked this below and connected this to our pi using HDMI and a usb for power. Upon booting when connected to the screen, the pi acts as a desktop with a web server and a terminal built in. The screen gives us access to bluetooth for audio as well. Some changes had to be made to files in our boot folder to get this to work, but implementing the screen was the first thing we needed to do.

Microphone: In order to get the microphone to work, it required a plug and play sound card. Once getting the soundcard and microphone hooked up together and working, the next step was to implement the voice recognition and text to speech libraries which were very straight forward. The final step was configuring the actual sensitivity of the microphone so that it would be optimal to pick up voices only in a close range to the mic.

Fingerprint Sensor: We implemented this using software and drivers that we obtained when purchasing the unit similar to how we set up the screen, but this was much more in depth. This had to be implemented using the built-in UART on the rx/tx pins and configured using the adafruit library. 

Speaker: We were able to connect this to our LCD screen using bluetooth. Because we implemented a ‘speech’ engine, when the chatbot outputs a result, it is played out by the speaker.

Keyboard/Mouse: These were plugged into the usb ports on our pi and made the readily available for use. 

UI: Our UI was the biggest challenge that we faced. This took multiple iterations and we were finally able to create one that implemented our touch ID sensor and our microphone successfully. We based this off of a gitHub repository we found on a youtube video. We used this code as the framework for our project. However, in order to produce the result we desired, there were a lot of changes that we had to implement. This included importing different libraries such as:
-adafruit_fingerprint which was for our fingerprint scanner
 	This is a fingerprint sensor library which is able to access a folder of downloaded and recognizable fingerprints, as well as having the capability to display the physical image of the fingerprint.
-openAI for the AI implementation using an API key
-speech_recognition, allowing for the user to prompt with speech
-pyttsx3, which was our text to speech conversion
  Works offline and uses an engine instance to recognize speech
-tkinter, which opened a popup window for our UI
  tkinter as tk is a TCL package implemented in C that displays GUI widgets
-dotenv, reads key value pairs from environment files to set them as variables,     implemented in our UI window
-time, used for setting delays and for how long the UI is displayed

Learning the syntax and the required functions for these libraries took the most amount of time, but after consistent testing, our UI functions as desired. Here is a screenshot of our implemented UI:

<img width="617" alt="Screenshot 2024-02-27 at 1 59 34 PM" src="https://github.com/sgmalik/OpenAI-RaspberryPi/assets/112639907/7d3a38d5-3865-429a-a93c-318e394cc7a5">


To summarize this project, we began with the idea of making something for schools that may not have access or a budget for a computer lab for their students. We came up with the idea of making an AI chatbot with voice recognition and a fingerprint sensor required for entry, so that it could be used by school faculty to give to students. To add complexity to this project, we wanted to add both the voice recognition aspect and the fingerprint sensor. After ordering of the components we needed, the first aspect to work on in terms of the physical components was the lcd screen. This component took a little bit longer to get working than we expected, as there were two ways you could use it, one being running the raspberry pi as a desktop off of the screen, which we opted for. Once we had that working, we wanted to start testing how the open api key would work and see if it would output to the terminal, which it did very easily. After having both the AI ready for implementation into a user interface, and the screen we were going to display it on working, the next and easily hardest step was creating a user interface. This user interface ended up taking 14 iterations to complete, and we even had to make a backup one a week before presenting because we fried our fingerprint sensor, which ended up being a huge element in the opening of the user interface. We wanted to make our user-interface fairly simple and didn’t want to over-complicate anything so we just had there be four buttons, one to enter the text the user has typed, one to click for speech input, a button to clear the output box if it got cluttered, and a button to exit the user interface. The UI took about three total weeks to complete, as we realized we would need to implement the other components into it before it could be fully complete and tested. Another hiccup in the process was working with the actual presentation and size of the screen that would appear, but once this was figured out, we finally had a user interface that was able to receive text input and give an output. Now all that was left to do was implement the microphone for voice input, and the fingerprint sensor to act as a barrier for accessing the chatbot. The microphone was one of the easier components to work with, the only thing we needed was to wait for a soundcard to arrive so that it could use voice recognition libraries and have the sensitivity of the mic adjusted to be optimal for picking up voices that were nearest it. The fingerprint sensor on the other hand was difficult to implement. It had to be used through the built-in UART rx/tx pins on the pi’s GPIO pins. This was a difficult process as the adafruit library proved quite tedious to get correct and working, as well as the actual sensing of the fingerprint part only working about 3/4s of the time. After these were implemented and working, the last part was to add some splash screens to the startup of the program to ask the user for their fingerprint, and two screens that could follow, one being the user accepted and one being the access denied screen. With this complete, the last and final step was testing it and working out bugs, which also took a bit longer than expected. 
